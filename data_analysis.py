# An√°lise Explorat√≥ria dos Dados - Amazon Titles Dataset
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from wordcloud import WordCloud
import logging
from config import DATA_CONFIG, CLEANING_CONFIG

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataAnalyzer:
    """
    Classe para an√°lise explorat√≥ria dos dados Amazon Titles
    """
    
    def __init__(self, file_path="trn.json"):
        self.file_path = file_path
        self.df = None
        self.analysis_results = {}
    
    def load_data(self):
        """Carrega os dados do arquivo JSON"""
        logger.info(f"Carregando dados de: {self.file_path}")
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.df = pd.DataFrame(data)
            logger.info(f"Dados carregados: {len(self.df)} registros")
            return True
            
        except Exception as e:
            logger.error(f"Erro ao carregar dados: {e}")
            return False
    
    def basic_statistics(self):
        """Estat√≠sticas b√°sicas do dataset"""
        logger.info("Calculando estat√≠sticas b√°sicas...")
        
        stats = {
            "total_records": len(self.df),
            "columns": list(self.df.columns),
            "missing_values": self.df.isnull().sum().to_dict(),
            "data_types": self.df.dtypes.to_dict()
        }
        
        # Estat√≠sticas de texto
        if 'title' in self.df.columns:
            stats["title_stats"] = {
                "mean_length": self.df['title'].str.len().mean(),
                "median_length": self.df['title'].str.len().median(),
                "min_length": self.df['title'].str.len().min(),
                "max_length": self.df['title'].str.len().max(),
                "unique_titles": self.df['title'].nunique()
            }
        
        if 'description' in self.df.columns:
            stats["description_stats"] = {
                "mean_length": self.df['description'].str.len().mean(),
                "median_length": self.df['description'].str.len().median(),
                "min_length": self.df['description'].str.len().min(),
                "max_length": self.df['description'].str.len().max(),
                "unique_descriptions": self.df['description'].nunique()
            }
        
        self.analysis_results["basic_stats"] = stats
        return stats
    
    def text_analysis(self):
        """An√°lise detalhada dos textos"""
        logger.info("Realizando an√°lise de texto...")
        
        text_analysis = {}
        
        # An√°lise de t√≠tulos
        if 'title' in self.df.columns:
            title_analysis = self._analyze_text_column(self.df['title'], "title")
            text_analysis["title"] = title_analysis
        
        # An√°lise de descri√ß√µes
        if 'description' in self.df.columns:
            desc_analysis = self._analyze_text_column(self.df['description'], "description")
            text_analysis["description"] = desc_analysis
        
        self.analysis_results["text_analysis"] = text_analysis
        return text_analysis
    
    def _analyze_text_column(self, series, column_name):
        """An√°lise detalhada de uma coluna de texto"""
        analysis = {
            "length_distribution": {
                "mean": series.str.len().mean(),
                "median": series.str.len().median(),
                "std": series.str.len().std(),
                "percentiles": series.str.len().quantile([0.25, 0.5, 0.75, 0.9, 0.95]).to_dict()
            },
            "word_count": {
                "mean": series.str.split().str.len().mean(),
                "median": series.str.split().str.len().median(),
                "max": series.str.split().str.len().max()
            },
            "unique_words": len(set(' '.join(series.dropna()).split())),
            "empty_texts": series.isna().sum() + (series == "").sum()
        }
        
        return analysis
    
    def create_visualizations(self):
        """Cria visualiza√ß√µes dos dados"""
        logger.info("Criando visualiza√ß√µes...")
        
        # Configurar estilo
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Distribui√ß√£o de comprimento dos t√≠tulos
        if 'title' in self.df.columns:
            title_lengths = self.df['title'].str.len()
            axes[0, 0].hist(title_lengths, bins=50, alpha=0.7, color='skyblue')
            axes[0, 0].set_title('Distribui√ß√£o do Comprimento dos T√≠tulos')
            axes[0, 0].set_xlabel('Comprimento')
            axes[0, 0].set_ylabel('Frequ√™ncia')
        
        # 2. Distribui√ß√£o de comprimento das descri√ß√µes
        if 'description' in self.df.columns:
            desc_lengths = self.df['description'].str.len()
            axes[0, 1].hist(desc_lengths, bins=50, alpha=0.7, color='lightcoral')
            axes[0, 1].set_title('Distribui√ß√£o do Comprimento das Descri√ß√µes')
            axes[0, 1].set_xlabel('Comprimento')
            axes[0, 1].set_ylabel('Frequ√™ncia')
        
        # 3. Palavras mais frequentes nos t√≠tulos
        if 'title' in self.df.columns:
            title_words = ' '.join(self.df['title'].dropna()).lower()
            word_freq = Counter(title_words.split())
            top_words = dict(word_freq.most_common(10))
            
            axes[1, 0].bar(top_words.keys(), top_words.values(), color='lightgreen')
            axes[1, 0].set_title('Top 10 Palavras nos T√≠tulos')
            axes[1, 0].set_xlabel('Palavras')
            axes[1, 0].set_ylabel('Frequ√™ncia')
            axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. Rela√ß√£o entre comprimento de t√≠tulo e descri√ß√£o
        if 'title' in self.df.columns and 'description' in self.df.columns:
            axes[1, 1].scatter(
                self.df['title'].str.len(), 
                self.df['description'].str.len(), 
                alpha=0.5, 
                color='purple'
            )
            axes[1, 1].set_title('Rela√ß√£o: Comprimento T√≠tulo vs Descri√ß√£o')
            axes[1, 1].set_xlabel('Comprimento do T√≠tulo')
            axes[1, 1].set_ylabel('Comprimento da Descri√ß√£o')
        
        plt.tight_layout()
        plt.savefig('data_analysis_plots.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        logger.info("Visualiza√ß√µes salvas em: data_analysis_plots.png")
    
    def generate_wordcloud(self):
        """Gera wordcloud dos textos"""
        logger.info("Gerando wordcloud...")
        
        # Wordcloud dos t√≠tulos
        if 'title' in self.df.columns:
            title_text = ' '.join(self.df['title'].dropna())
            wordcloud_title = WordCloud(
                width=800, 
                height=400, 
                background_color='white',
                max_words=100
            ).generate(title_text)
            
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud_title, interpolation='bilinear')
            plt.axis('off')
            plt.title('Wordcloud - T√≠tulos')
            plt.savefig('wordcloud_titles.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # Wordcloud das descri√ß√µes
        if 'description' in self.df.columns:
            desc_text = ' '.join(self.df['description'].dropna())
            wordcloud_desc = WordCloud(
                width=800, 
                height=400, 
                background_color='white',
                max_words=100
            ).generate(desc_text)
            
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud_desc, interpolation='bilinear')
            plt.axis('off')
            plt.title('Wordcloud - Descri√ß√µes')
            plt.savefig('wordcloud_descriptions.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        logger.info("Wordclouds salvos")
    
    def data_quality_report(self):
        """Gera relat√≥rio de qualidade dos dados"""
        logger.info("Gerando relat√≥rio de qualidade...")
        
        quality_report = {
            "missing_data": self.df.isnull().sum().to_dict(),
            "duplicate_records": self.df.duplicated().sum(),
            "empty_texts": {
                "title": (self.df['title'] == "").sum() if 'title' in self.df.columns else 0,
                "description": (self.df['description'] == "").sum() if 'description' in self.df.columns else 0
            },
            "text_length_issues": {
                "very_short_titles": (self.df['title'].str.len() < 5).sum() if 'title' in self.df.columns else 0,
                "very_long_titles": (self.df['title'].str.len() > 200).sum() if 'title' in self.df.columns else 0,
                "very_short_descriptions": (self.df['description'].str.len() < 10).sum() if 'description' in self.df.columns else 0,
                "very_long_descriptions": (self.df['description'].str.len() > 1000).sum() if 'description' in self.df.columns else 0
            }
        }
        
        self.analysis_results["quality_report"] = quality_report
        return quality_report
    
    def save_analysis_report(self, filename="data_analysis_report.json"):
        """Salva relat√≥rio completo da an√°lise"""
        logger.info(f"Salvando relat√≥rio em: {filename}")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        
        logger.info("Relat√≥rio salvo com sucesso")
    
    def print_summary(self):
        """Imprime resumo da an√°lise"""
        print("\n" + "="*50)
        print("RELAT√ìRIO DE AN√ÅLISE - AMAZON TITLES DATASET")
        print("="*50)
        
        if "basic_stats" in self.analysis_results:
            stats = self.analysis_results["basic_stats"]
            print(f"\nüìä ESTAT√çSTICAS B√ÅSICAS:")
            print(f"   Total de registros: {stats['total_records']:,}")
            print(f"   Colunas: {', '.join(stats['columns'])}")
            
            if 'title' in stats:
                title_stats = stats['title_stats']
                print(f"\nüìù AN√ÅLISE DOS T√çTULOS:")
                print(f"   Comprimento m√©dio: {title_stats['mean_length']:.1f} caracteres")
                print(f"   Comprimento mediano: {title_stats['median_length']:.1f} caracteres")
                print(f"   T√≠tulos √∫nicos: {title_stats['unique_titles']:,}")
            
            if 'description' in stats:
                desc_stats = stats['description_stats']
                print(f"\nüìÑ AN√ÅLISE DAS DESCRI√á√ïES:")
                print(f"   Comprimento m√©dio: {desc_stats['mean_length']:.1f} caracteres")
                print(f"   Comprimento mediano: {desc_stats['median_length']:.1f} caracteres")
                print(f"   Descri√ß√µes √∫nicas: {desc_stats['unique_descriptions']:,}")
        
        if "quality_report" in self.analysis_results:
            quality = self.analysis_results["quality_report"]
            print(f"\nüîç QUALIDADE DOS DADOS:")
            print(f"   Registros duplicados: {quality['duplicate_records']:,}")
            print(f"   T√≠tulos vazios: {quality['empty_texts']['title']:,}")
            print(f"   Descri√ß√µes vazias: {quality['empty_texts']['description']:,}")
        
        print("\n" + "="*50)

def main():
    """Fun√ß√£o principal para executar an√°lise"""
    logger.info("=== INICIANDO AN√ÅLISE EXPLORAT√ìRIA ===")
    
    # Inicializar analisador
    analyzer = DataAnalyzer()
    
    # Carregar dados
    if not analyzer.load_data():
        logger.error("Falha ao carregar dados. Encerrando.")
        return
    
    # Executar an√°lises
    analyzer.basic_statistics()
    analyzer.text_analysis()
    analyzer.data_quality_report()
    
    # Criar visualiza√ß√µes
    try:
        analyzer.create_visualizations()
        analyzer.generate_wordcloud()
    except Exception as e:
        logger.warning(f"Erro ao criar visualiza√ß√µes: {e}")
    
    # Salvar relat√≥rio
    analyzer.save_analysis_report()
    
    # Imprimir resumo
    analyzer.print_summary()
    
    logger.info("=== AN√ÅLISE CONCLU√çDA ===")

if __name__ == "__main__":
    main() 